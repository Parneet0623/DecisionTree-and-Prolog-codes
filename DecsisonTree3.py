# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k4thESQmcwDKtNeabO37eU_K3tpHVR5n
"""

# Import the required libraries
import pandas as pd
import numpy as np

pima = pd.read_csv('Book1.csv')
pima.head()

# Define the calculate entropy function
def calculate_entropy(pima_label):
    classes,class_counts = np.unique(pima_label,return_counts = True)
    entropy_value = np.sum([(-class_counts[i]/np.sum(class_counts))*np.log2(class_counts[i]/np.sum(class_counts)) 
                        for i in range(len(classes))])
    return entropy_value

def calculate_information_gain(dataset,feature,label): 
    # Calculate the dataset entropy
    dataset_entropy = calculate_entropy(dataset[label])   
    values,feat_counts= np.unique(dataset[feature],return_counts=True)
    
    # Calculate the weighted feature entropy                                # Call the calculate_entropy function
    weighted_feature_entropy = np.sum([(feat_counts[i]/np.sum(feat_counts))*calculate_entropy(dataset.where(dataset[feature]
                              ==values[i]).dropna()[label]) for i in range(len(values))])    
    feature_info_gain = dataset_entropy - weighted_feature_entropy
    return feature_info_gain

# Define the create decision tree function
def create_decision_tree(dataset,pima,features,label,parent):

  datum = np.unique(pima[label],return_counts=True)
  unique_data = np.unique(dataset[label])

  if len(unique_data) <= 1:
    return unique_data[0]

  elif len(dataset) == 0:
    return unique_data[np.argmax(datum[1])]

  elif len(features) == 0:
    return parent

  else:
    parent = unique_data[np.argmax(datum[1])]

    # call the calculate_info_gain function
    item_values = [calculate_information_gain(dataset,feature,label) for feature in features]

    optimum_feature_index = np.argmax(item_values)
    optimum_feature = features[optimum_feature_index]
    decision_tree = {optimum_feature:{}}
    features = [i for i in features if i != optimum_feature]

    for value in np.unique(dataset[optimum_feature]):
      min_data = dataset.where(dataset[optimum_feature] == value).dropna()

      # recursive call to create_decision_tree function
      min_tree =create_decision_tree(min_data,pima,features,label,parent)

      decision_tree[optimum_feature][value] = min_tree

    return(decision_tree)

# define predict function
def predict_diagnosis(test_data,decision_tree):

  for nodes in decision_tree.keys():
    value = test_data[nodes]
    decision_tree = decision_tree[nodes][value]

    prediction = 0
    if type(decision_tree) is dict:


      #recusive call to predict_diagnosis function
      prediction = predict_diagnosis(test_data, decision_tree)
    else:
      prediction = decision_tree
      break;
      
  return prediction

# Set the features and label
features = pima.columns[:-1]
label = 'Diagnosis'
parent=None
features

# Train the decision tree model
decision_tree = create_decision_tree(pima,pima,features,label,parent)

# Predict using the trained model
sample_data = {'Sore Throat':1,'Fever':0,'Swollen Gland':1,'Congestion':1,'Headache':0 }
test_data = pd.Series(sample_data)
prediction = predict_diagnosis(test_data,decision_tree)
print("prediction for given sample data is: ",prediction)















